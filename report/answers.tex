\documentclass[12pt, fleqn]{article}

\usepackage[left=0.75in, right=0.75in, bottom=0.75in, top=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{floatrow}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{sectsty}
\usepackage{enumitem}
\renewcommand{\thesubsubsection}{(\alph{subsubsection})}

\usepackage[dvipsnames]{xcolor}
\usepackage[perpage]{footmisc}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{190100044}
\rhead{Assignment 3}
\renewcommand{\footrulewidth}{1.0pt}
\cfoot{Page \thepage}

\setlength{\parindent}{0em}

\title{Assignment 3}
\author{Devansh Jain, 190100044}
\date{17th Oct 2021}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

% \pagenumbering{gobble}
\maketitle
\tableofcontents
\thispagestyle{empty}
\setcounter{page}{0}

\newpage

\section{Logistic Regression}
\subsection{CS 337: Logistic Regression}

Given Soft-max expression and Categorical cross-entropy function of Multi-class Logistic Regression, we substitute $K = 2$.
\begin{equation*}
  \begin{aligned}
    P(Y = 1| \mathbf{w}_1, \phi(\mathbf{x})) & = \frac{e^{\mathbf{w}_1^T \phi(\mathbf{x})}}{e^{\mathbf{w}_1^T \phi(\mathbf{x})} + e^{\mathbf{w}_2^T \phi(\mathbf{x})}} = \frac{1}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}                                                      \\
    P(Y = 2| \mathbf{w}_2, \phi(\mathbf{x})) & = \frac{e^{\mathbf{w}_2^T \phi(\mathbf{x})}}{e^{\mathbf{w}_1^T \phi(\mathbf{x})} + e^{\mathbf{w}_2^T \phi(\mathbf{x})}} = \frac{e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}} \\
    E(\mathbf{w}_1, \mathbf{w}_2)            & = - \frac{1}{N} \sum_{i=1}^N y_1^{(i)} \log (P(Y = 1| \mathbf{w}_1, \phi(\mathbf{x}^{(i)}))) + y_2^{(i)} \log (P(Y = 2| \mathbf{w}_2, \phi(\mathbf{x}^{(i)})))                                                                                      \\
                                             & = - \frac{1}{N} \sum_{i=1}^N \frac{y_1^{(i)} + y_2^{(i)} e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}                                                                        \\
  \end{aligned}
\end{equation*}

Now, we substitute the one-hot vector $[y_1^{(i)}, y_2^{(i)}]$ with $[y^{(i)}, 1 - y^{(i)}]$ and $\mathbf{w}_1 - \mathbf{w}_2$ with $\mathbf{w}$.
\begin{equation*}
  \begin{aligned}
    P(Y = 1| \mathbf{w}, \phi(\mathbf{x})) & = \frac{1}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} = \sigma_\mathbf{w} (\mathbf{x})                                                                                                   \\
    P(Y = 2| \mathbf{w}, \phi(\mathbf{x})) & = \frac{e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} = 1 - \sigma_\mathbf{w} (\mathbf{x})                                                              \\
    E(\mathbf{w})                          & = - \frac{1}{N} \sum_{i=1}^N \frac{y^{(i)} + (1 - y^{(i)}) e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}}                                                \\
                                           & = - \frac{1}{N} \sum_{i=1}^N \frac{y^{(i)}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} + \frac{(1 - y^{(i)}) e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} \\
                                           & = - \frac{1}{N} \sum_{i=1}^N y^{(i)} \sigma_\mathbf{w} (\mathbf{x}) + (1 - y^{(i)})(1 - \sigma_\mathbf{w} (\mathbf{x}))                                                               \\
  \end{aligned}
\end{equation*}

Hence, cross entropy function used to train a binary logistic regression is a special case of the categorical cross entropy function given above.


\subsection{CS 337: Logistic Regressionâ€™s Decision surface}
The model assigns a value of $y = +1$ to a point $\mathbf{x}$ if $P(y = +1 | \mathbf{x}) = \dfrac{1}{1 + \exp (-\mathbf{w}^T \mathbf{x})} \ge \theta$ which is equivalent to $\mathbf{w}^T \mathbf{x} \ge - \log \bigg( \dfrac{1 - \theta}{\theta} \bigg)$ and this forms the decision boundary.

When we say that a model is linear, we mean that its predictions are a linear function of its parameters. \\
Logistic regression is considered as a linear model because the \textbf{decision boundary used for classification purpose is linear} (i.e. $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}$).


\subsection{CS 337: Multi Class Logistic Regression}
\subsubsection{}
Assuming that we don't lower case all letters during featurization: \\
Let `The' be the $j_1$th word, `food' be the $j_2$th word, `in' be the $j_3$th word, `the' be the $j_4$th word, `restaurant' be the $j_5$th word, `tastes' be the $j_6$th word and, `good' be the $j_7$th word in the vocabulary. \\
$\phi_j(\text{``The food in the restaurant tastes good"}) = 1$ if $j \in \{j_1, j_2, j_3, j_4, j_5, j_6, j_7 \}$ else $0$.

\medskip

Assuming that we lower case all letters during featurization: \\
Let `the' be the $j_1$th word, `food' be the $j_2$th word, `in' be the $j_3$th word, `restaurant' be the $j_4$th word, `tastes' be the $j_5$th word and, `good' be the $j_6$th word in the vocabulary. \\
$\phi_j(\text{``The food in the restaurant tastes good"}) = 1$ if $j \in \{j_1, j_2, j_3, j_4, j_5, j_6 \}$ else $0$.

\subsubsection{}
Few notable limitations or drawbacks of this featurization technique:
\begin{itemize}
  \item Lose of order: We lose the order of words which might be important in determining the meaning and sentiment of the text. \\
        Example: ``It tastes bad, not good" and ``It tastes good, not bad"

  \item Size of dictionary: Dictionary has a lot of words when compared to a text sample, thus the feature matrix which we obtain is sparse. \\
        It leads to inefficiency in computation.

  \item In the specified technique, we aren't applying stemming or lower casing of words or removing stop words. All these help in decreasing the dictionary size a lot.
\end{itemize}

\subsubsection{}
We would have to learn 3 vectors $w_-, w_0, w_+$, each of length 1000 (feature vector length = vocabulary size). Thus, we require to learn 3000 parameters.

\subsubsection{}
If we use sum normalization on [1, 2, 3] and [100, 200, 300], we get the same probabilities. \\
However, as we have more resolution in the latter case, we should judge that the last class is the more probable than it was in the former case. \\
This is taken care by the soft-max function.

If we try to normalize following unnormalized values [1, 2, 3] and [101, 102, 103], we should get same probabilities as it is just shifted by constant. \\
However, we get probability of last class as 0.5 in former case and 0.34 in latter case. \\
This is taken care by the soft-max function.

\subsubsection{}
(i) Posterior values are sum normalization are 0.04, 0.19, 0.77. \\
(ii) Posterior values for soft-max normalization are 0.11, 0.16, 0.73. \\
Values for $y=0$ increases in soft-max while the values of $y=1$ and $y=2$ decreases. \\
Usually soft-max is a better approximate of hard-max but here the values are close to 1. \\

\medskip
\textit{Note: For \textbf{(f)} to \textbf{(l)}, assuming that $y_i = k$ means that the point $x_i$ is assigned class $k$, where $k \in \{1, \dots, K\}$. $W = \{w_k : k \in \{1, \dots, K\}  \}$ where $w_k$ is the weight vector for class $k$.}

\subsubsection{}
\begin{equation*}
  \begin{aligned}
    \mathbb{P}(D|W) & = \mathbb{P}(\{(x_i, y_i)\}_{i=1}^n | \{w_k\}_{k=1}^K)                                                                                    \\
                    & = \prod_{i=1}^n \mathbb{P}((x_i, y_i) | \{w_k\}_{k=1}^K)                                                                                  \\
                    & = \prod_{i=1}^n \prod_{k=1}^K \mathbb{P}_W (y_i = k | x_i)^{\mathcal{I}(y_i = k)}                                                         \\
                    & = \prod_{i=1}^n \prod_{k=1}^K \bigg( \frac{\exp (w_k^T \phi(x_i))}{\sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i))} \bigg)^{\mathcal{I}(y_i = k)} \\
                    & = \prod_{i=1}^n \frac{\prod_{k=1}^K (\exp (w_k^T \phi(x_i)))^{\mathcal{I}(y_i = k)}}{\sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i))}
  \end{aligned}
\end{equation*}

\subsubsection{}
\begin{equation*}
  \begin{aligned}
    \log (\mathbb{P}(D|W)) & = \sum_{i=1}^n \bigg( \sum_{k=1}^K \mathcal{I}(y_i = k) (w_k^T \phi(x_i)) - \log \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg) \bigg)                            \\
                           & = \bigg[ \sum_{i=1}^n \sum_{k=1}^K \mathcal{I}(y_i = k) (w_k^T \phi(x_i)) \bigg] - \bigg[ \sum_{i=1}^n \log \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg) \bigg] \\
                           & = Numer - Denom                                                                                                                                                          \\
    \\
    Numer                  & = \sum_{i=1}^n \sum_{k=1}^K \mathcal{I}(y_i = k) (w_k^T \phi(x_i))                                                                                                       \\
    Denom                  & = \sum_{i=1}^n \log \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)
  \end{aligned}
\end{equation*}

\subsubsection{}
$w_{j,k}$ is the weight of $j$th feature for class $k$.
\begin{equation*}
  \begin{aligned}
    \frac{\partial Numer}{\partial w_{j,k}} & = \frac{\partial}{\partial w_{j,k}} \bigg( \sum_{i=1}^n \sum_{k'=1}^K \mathcal{I}(y_i = k') (w_{k'}^T \phi(x_i)) \bigg) \\
                                            & = \sum_{i=1}^n \frac{\partial}{\partial w_{j,k}} \bigg( \mathcal{I}(y_i = k) (w_k^T \phi(x_i)) \bigg)                   \\
                                            & = \sum_{i=1}^n \mathcal{I}(y_i = k) \frac{\partial}{\partial w_{j,k}} \bigg( w_{j,k} \phi_j(x_i) \bigg)                 \\
                                            & = \sum_{i=1}^n \mathcal{I}(y_i = k) \phi_j(x_i)
  \end{aligned}
\end{equation*}

\subsubsection{}
$w_{j,k}$ is the weight of $j$th feature for class $k$.
\begin{equation*}
  \begin{aligned}
    \frac{\partial Denom}{\partial w_{j,k}} & = \frac{\partial}{\partial w_{j,k}} \bigg( \sum_{i=1}^n \log \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg) \bigg)                                                 \\
                                            & = \sum_{i=1}^n \frac{\partial}{\partial w_{j,k}} \bigg( \log \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg) \bigg)                                                 \\
                                            & = \sum_{i=1}^n \frac{1}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \frac{\partial}{\partial w_{j,k}} \bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)    \\
                                            & = \sum_{i=1}^n \frac{1}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \frac{\partial}{\partial w_{j,k}} \bigg( \exp (w_{k}^T \phi(x_i)) \bigg)                   \\
                                            & = \sum_{i=1}^n \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \frac{\partial}{\partial w_{j,k}} \bigg( w_{j,k} \phi_j(x_i) \bigg) \\
                                            & = \sum_{i=1}^n \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \phi_j(x_i)
  \end{aligned}
\end{equation*}

\subsubsection{}
\begin{equation*}
  \begin{aligned}
    \frac{\partial \log (\mathbb{P}(D|W)}{\partial w_{j,k}} & = \frac{\partial (Numer - Denom)}{\partial w_{j,k}}                                                                                                                                           \\
                                                            & = \bigg[ \sum_{i=1}^n \mathcal{I}(y_i = k) \phi_j(x_i) \bigg] - \bigg[ \sum_{i=1}^n \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \phi_j(x_i) \bigg] \\
                                                            & = \sum_{i=1}^n \bigg[ \mathcal{I}(y_i = k) \phi_j(x_i) - \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \bigg] \phi_j(x_i)
  \end{aligned}
\end{equation*}

\subsubsection{}
\begin{equation*}
  \begin{aligned}
     & \forall \ j,k;  \frac{\partial \log (\mathbb{P}(D|W)}{\partial w_{j,k}} = 0 \ \bigg\vert_{W = W^*}                                                                                           \\
     & \forall \ j,k;  \sum_{i=1}^n \mathcal{I}(y_i = k) \phi_j(x_i) = \sum_{i=1}^n \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \phi_j(x_i)              \\
     & \forall \ k \in \{1, \dots, K\}; \sum_{i=1}^n \mathcal{I}(y_i = k) \phi(x_i) = \sum_{i=1}^n \frac{\exp (w_{k}^T \phi(x_i))}{\bigg( \sum_{k'=1}^K \exp (w_{k'}^T \phi(x_i)) \bigg)} \phi(x_i) \\
     & \forall \ k \in \{1, \dots, K\}; \sum_{i=1}^n \mathcal{I} (y_i = k) \phi(x_i) = \sum_{i=1}^n \mathbb{P}_{W^*} (y_i = k | x_i) \phi(x_i)
  \end{aligned}
\end{equation*}
These are the Balance Equations.

\subsubsection{}
The equations mean ``For optimal $W^*$, the sum of feature vectors belonging to a given class $k$ is equal to the weighted sum of all feature vectors where weights are the probability of the point belonging to class $k$ as predicted by the model".
\begin{equation*}
  \begin{aligned}
     & \forall \ k \in \{1, \dots, K\}; \sum_{i=1}^n \mathcal{P}_D (y_i = k | x_i) \phi(x_i) = \sum_{i=1}^n \mathbb{P}_{W^*} (y_i = k | x_i) \phi(x_i) \\
     & \forall \ k \in \{1, \dots, K\}; \ E_D [\phi(X) | Y = k] = E_{W^*} [\phi(X) | Y = k]
  \end{aligned}
\end{equation*}
where, $E_D$ is the expectation over the true training data distribution and $E_{W^*}$ is the expectation over our modelâ€™s predicted distribution.


\newpage
\subsection{CS 335: Lab Problems}
\subsubsection{}
\begin{enumerate}[label=(\roman*)]
  \item Code for the functions \verb!normalize()!, \verb!scaling()! and \verb!split_data! updated in notebook. \\
        \verb!split_data! works better with scaling than normalize.
  \item Code for the function \verb!get_data_for_class()! updated in notebook.
  \item Code for the function \verb!sample_training_points()! updated in notebook.
  \item Code for the functions \verb!sigmoid()!, \verb!cross_entropy_loss()! and \verb!grad()! updated in notebook.
  \item Code for the function \verb!logistic_regression()! updated in notebook.
  \item Code for the function \verb!prediction()! updated in notebook.
  \item Code for the function \verb!accuracy()! updated in notebook.
\end{enumerate}

\subsubsection{}
Trained classifier: \\
Hyper-parameters used - \texttt{epochs=500, lr=0.1, sample\_size=600} \\
\begin{tabular}{|c||c||}
  \hline
          & Accuracy \\
  \hline
  Total   & 0.9425   \\
  Class 1 & 1.0      \\
  Class 4 & 0.9063   \\
  Class 7 & 0.8958   \\
  Class 9 & 0.9612   \\
  \hline
\end{tabular} \\

Model M: \\
\begin{tabular}{|c||c||}
  \hline
          & Accuracy \\
  \hline
  Total   & 0.2625   \\
  Class 1 & 1.0      \\
  Class 4 & 0.0      \\
  Class 7 & 0.0      \\
  Class 9 & 0.0      \\
  \hline
\end{tabular} \\

Class based accuracy is not a good metrics as even though there are a lot of false positives for class 1 in model M, it gives accuracy of 1. \\
And total accuracy of Model M is fraction of images with true value 1. If this number was high then we would get a higher total accuracy without changing the model (which obviously is a really bad model)

However, here as the classes have similar number of images, total accuracy is a good metrics. \\

\subsubsection{}
Code for the function \verb!calculate_metrics()! updated in notebook. \\

Trained classifier: \\
Hyper-parameters used - \texttt{epochs=500, lr=0.1, sample\_size=600} \\
\begin{tabular}{|c||c|c|c||}
  \hline
          & Precision & Recall & F1-score \\
  \hline
  Total   & 0.9451    & 0.9425 & 0.9425   \\
  Class 1 & 0.9633    & 1.0000 & 0.9813   \\
  Class 4 & 0.9886    & 0.9063 & 0.9457   \\
  Class 7 & 0.9556    & 0.8958 & 0.9247   \\
  Class 9 & 0.8761    & 0.9612 & 0.9167   \\
  \hline
\end{tabular} \\

Model M: \\
\begin{tabular}{|c||c|c|c||}
  \hline
          & Precision & Recall & F1-score \\
  \hline
  Total   & 0.0689    & 0.2625 & 0.1092   \\
  Class 1 & 0.2625    & 1.0000 & 0.4158   \\
  Class 4 & 0.0000    & 0.0000 & 0.0000   \\
  Class 7 & 0.0000    & 0.0000 & 0.0000   \\
  Class 9 & 0.0000    & 0.0000 & 0.0000   \\
  \hline
\end{tabular} \\

Recall is same as accuracy. Precision takes into account false positives. This makes F1-score a much better than just accuracy.

\medskip

\textit{Note: To avoid division by zero, I have assumed that the precision, recall and F1-score is 0 if count of true positives is 0.}



\end{document}
