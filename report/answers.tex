\documentclass[12pt, fleqn]{article}

\usepackage[left=0.75in, right=0.75in, bottom=0.75in, top=1.0in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{floatrow}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{sectsty}
\renewcommand{\thesubsubsection}{(\alph{subsubsection})}

\usepackage[dvipsnames]{xcolor}
\usepackage[perpage]{footmisc}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{190100044}
\rhead{Assignment 3}
\renewcommand{\footrulewidth}{1.0pt}
\cfoot{Page \thepage}

\setlength{\parindent}{0em}

\title{Assignment 3}
\author{Devansh Jain, 190100044}
\date{17th Oct 2021}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

% \pagenumbering{gobble}
\maketitle
\tableofcontents
\thispagestyle{empty}
\setcounter{page}{0}

\newpage

\section{Logistic Regression}
\subsection{CS 337: Logistic Regression}

Given Soft-max expression and Categorical cross-entropy function of Multi-class Logistic Regression, we substitute $K = 2$.
\begin{equation*}
  \begin{aligned}
    P(Y = 1| \mathbf{w}_1, \phi(\mathbf{x})) & = \frac{e^{\mathbf{w}_1^T \phi(\mathbf{x})}}{e^{\mathbf{w}_1^T \phi(\mathbf{x})} + e^{\mathbf{w}_2^T \phi(\mathbf{x})}} = \frac{1}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}                                                      \\
    P(Y = 2| \mathbf{w}_2, \phi(\mathbf{x})) & = \frac{e^{\mathbf{w}_2^T \phi(\mathbf{x})}}{e^{\mathbf{w}_1^T \phi(\mathbf{x})} + e^{\mathbf{w}_2^T \phi(\mathbf{x})}} = \frac{e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}} \\
    E(\mathbf{w}_1, \mathbf{w}_2)            & = - \frac{1}{N} \sum_{i=1}^N y_1^{(i)} \log (P(Y = 1| \mathbf{w}_1, \phi(\mathbf{x}^{(i)}))) + y_2^{(i)} \log (P(Y = 2| \mathbf{w}_2, \phi(\mathbf{x}^{(i)})))                                                                                      \\
                                             & = - \frac{1}{N} \sum_{i=1}^N \frac{y_1^{(i)} + y_2^{(i)} e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}{1 + e^{(\mathbf{w}_2^T - \mathbf{w}_1^T) \phi(\mathbf{x})}}                                                                        \\
  \end{aligned}
\end{equation*}

Now, we substitute the one-hot vector $[y_1^{(i)}, y_2^{(i)}]$ with $[y^{(i)}, 1 - y^{(i)}]$ and $\mathbf{w}_1 - \mathbf{w}_2$ with $\mathbf{w}$.
\begin{equation*}
  \begin{aligned}
    P(Y = 1| \mathbf{w}, \phi(\mathbf{x})) & = \frac{1}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} = \sigma_\mathbf{w} (\mathbf{x})                                                                                                   \\
    P(Y = 2| \mathbf{w}, \phi(\mathbf{x})) & = \frac{e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} = 1 - \sigma_\mathbf{w} (\mathbf{x})                                                              \\
    E(\mathbf{w})                          & = - \frac{1}{N} \sum_{i=1}^N \frac{y^{(i)} + (1 - y^{(i)}) e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}}                                                \\
                                           & = - \frac{1}{N} \sum_{i=1}^N \frac{y^{(i)}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} + \frac{(1 - y^{(i)}) e^{-\mathbf{w}^T \phi(\mathbf{x})}}{1 + e^{-\mathbf{w}^T \phi(\mathbf{x})}} \\
                                           & = - \frac{1}{N} \sum_{i=1}^N y^{(i)} \sigma_\mathbf{w} (\mathbf{x}) + (1 - y^{(i)})(1 - \sigma_\mathbf{w} (\mathbf{x}))                                                               \\
  \end{aligned}
\end{equation*}

Hence, cross entropy function used to train a binary logistic regression is a special case of the categorical cross entropy function given above.


\subsection{CS 337: Logistic Regressionâ€™s Decision surface}
The model assigns a value of $y = +1$ to a point $\mathbf{x}$ if $P(y = +1 | \mathbf{x}) = \dfrac{1}{1 + \exp (-\mathbf{w}^T \mathbf{x})} \ge \theta$ which is equivalent to $\mathbf{w}^T \mathbf{x} \ge - \log \bigg( \dfrac{1 - \theta}{\theta} \bigg)$ and this forms the decision boundary.

When we say that a model is linear, we mean that its predictions are a linear function of its parameters. \\
Logistic regression is considered as a linear model because the \textbf{decision boundary used for classification purpose is linear} (i.e. $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}$).


\subsection{CS 337: Multi Class Logistic Regression}
\subsubsection{}
Assuming that we don't lower case all letters during featurization: \\
Let `The' be the $j_1$th word, `food' be the $j_2$th word, `in' be the $j_3$th word, `the' be the $j_4$th word, `restaurant' be the $j_5$th word, `tastes' be the $j_6$th word and, `good' be the $j_7$th word in the vocabulary. \\
$\phi_j(\text{``The food in the restaurant tastes good"}) = 1$ if $j \in \{j_1, j_2, j_3, j_4, j_5, j_6, j_7 \}$ else $0$.

\medskip

Assuming that we lower case all letters during featurization: \\
Let `the' be the $j_1$th word, `food' be the $j_2$th word, `in' be the $j_3$th word, `restaurant' be the $j_4$th word, `tastes' be the $j_5$th word and, `good' be the $j_6$th word in the vocabulary. \\
$\phi_j(\text{``The food in the restaurant tastes good"}) = 1$ if $j \in \{j_1, j_2, j_3, j_4, j_5, j_6 \}$ else $0$.

\subsubsection{}
Few notable limitations or drawbacks of this featurization technique:
\begin{itemize}
  \item Lose of order: We lose the order of words which might be important in determining the meaning and sentiment of the text. \\
        Example: ``It tastes bad, not good" and ``It tastes good, not bad"

  \item Size of dictionary: Dictionary has a lot of words when compared to a text sample, thus the feature matrix which we obtain is sparse. \\
        It leads to inefficiency in computation.

  \item In the specified technique, we aren't applying stemming or lower casing of words or removing stop words. All these help in decreasing the dictionary size a lot.
\end{itemize}

\subsubsection{}
We would have to learn 3 vectors $w_-, w_0, w_+$, each of length 1000 (feature vector length = vocabulary size). Thus, we require to learn 3000 parameters.

\subsubsection{}
If we use sum normalization on [1, 2, 3] and [100, 200, 300], we get the same probabilities. \\
However, as we have more resolution in the latter case, we should judge that the last class is the more probable than it was in the former case. \\
This is taken care by the soft-max function.

If we try to normalize following unnormalized values [1, 2, 3] and [101, 102, 103], we should get same probabilities as it is just shifted by constant. \\
However, we get probability of last class as 0.5 in former case and 0.34 in latter case. \\
This is taken care by the soft-max function.

\subsubsection{}
(i) Posterior values are sum normalization are 0.04, 0.19, 0.77. \\
(ii) Posterior values for soft-max normalization are 0.11, 0.16, 0.73. \\
Values for $y=0$ increases in soft-max while the values of $y=1$ and $y=2$ decreases. \\
Usually soft-max is a better approximate of hard-max but here the values are close to 1.



\end{document}
