{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "## <font color=red> Please don't rename this .ipynb file.</font><br>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between\n",
    "\n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxgQgosmYXRu"
   },
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSk_qe4RXFYz"
   },
   "outputs": [],
   "source": [
    "X = np.load(\"./data/train_X.npy\")\n",
    "Y = np.array([np.load(\"./data/train_y.npy\")]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "## Normalization / Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input image features\n",
    "\n",
    "    Args:\n",
    "    X : numpy array of shape (n_samples, 784)\n",
    "\n",
    "    Returns:\n",
    "    X_norm : numpy array of shape (n_samples, 784) after normalization\n",
    "    \"\"\"\n",
    "\n",
    "    X_norm = None\n",
    "\n",
    "    ## TODO\n",
    "    X_norm = np.zeros(X.shape)\n",
    "    nf = X.shape[1]\n",
    "\n",
    "    for j in range(nf):\n",
    "        mean = X[:, j].mean()\n",
    "        std = X[:, j].std()\n",
    "        if std != 0:\n",
    "            X_norm[:, j] = (X[:, j] - mean) / std\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_norm.shape == X.shape\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "\n",
    "def scaling(X):\n",
    "    \"\"\"\n",
    "    Implement MinMax Scaling on input image features\n",
    "\n",
    "    Args:\n",
    "    X : numpy array of shape (n_samples, 784)\n",
    "\n",
    "    Returns:\n",
    "    X_scaled : numpy array of shape (n_samples, 784)\n",
    "    \"\"\"\n",
    "\n",
    "    X_scaled = None\n",
    "\n",
    "    ## TODO\n",
    "    X_scaled = np.zeros(X.shape)\n",
    "    nf = X.shape[1]\n",
    "\n",
    "    for j in range(nf):\n",
    "        high = X[:, j].max()\n",
    "        low = X[:, j].min()\n",
    "        if high != low:\n",
    "            X_scaled[:, j] = (X[:, j] - low) / (high - low)\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_scaled.shape == X.shape\n",
    "\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([[4, 2, 1], [5, 3, 1], [10, 4, 1], [10, 4, 1]], dtype=\"float64\")\n",
    "S.shape, normalize(S), scaling(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split data into train and validation sets\n",
    "    The first floor(train_ratio*n_sample) samples form the train set\n",
    "    and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    \"\"\"\n",
    "\n",
    "    # Try Normalization and scaling and store it in X_transformed\n",
    "    # X_transformed = X\n",
    "\n",
    "    ## TODO\n",
    "    # X_transformed = normalize(X)\n",
    "    X_transformed = scaling(X)\n",
    "    ## END TODO\n",
    "\n",
    "    assert X_transformed.shape == X.shape\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train, X_val, Y_val = (\n",
    "        X_transformed[train_indices],\n",
    "        Y[train_indices],\n",
    "        X_transformed[val_indices],\n",
    "        Y[val_indices],\n",
    "    )\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT-b4s1Kb6eL"
   },
   "source": [
    "**Plotting image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jlAZLOtY73s"
   },
   "outputs": [],
   "source": [
    "def plot_image(x, y, idx):\n",
    "    \"\"\"\n",
    "    Plot the given feature vector into image of size 28 X 28\n",
    "    note that originally the image was of size 28*28 which is flattened or unrolled\n",
    "    to 784 X 1 feature vector\n",
    "\n",
    "    Args:\n",
    "    x : numpy array of images\n",
    "    y : numpy array of ground truth labels for images\n",
    "    idx : index of the image\n",
    "    \"\"\"\n",
    "\n",
    "    image = np.reshape(x[idx], (28, 28))\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "    print(f\"The ground truth label for this iamge is : {y[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkI57T5zdsci"
   },
   "source": [
    "Encode 1 for any one of the class and 0 to all other remaining class for all labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlRACMv1WggK"
   },
   "outputs": [],
   "source": [
    "def get_data_for_class(X, Y, id):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X : numpy array of input features, shape - (n_samples, 784)\n",
    "    Y : numpy array of input targets, shape - (n_samples, 1)\n",
    "    id : class id (one of 1,4,7,9)\n",
    "\n",
    "    Returns:\n",
    "    class_X : numpy array of input features, shape - (n_samples, 784)\n",
    "    class_Y : numpy array of input targets, where class_Y[i]=1 if Y[i]=id else class_Y[i]=0, shape - (n_samples, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    class_X, class_Y = None, None\n",
    "\n",
    "    ## TODO\n",
    "    class_X = X\n",
    "    class_Y = Y == id\n",
    "    ## END TODO\n",
    "\n",
    "    assert class_X.shape == X.shape and class_Y.shape == Y.shape\n",
    "\n",
    "    return class_X, class_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFhZGe-2djbu"
   },
   "source": [
    "Random Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKCNwUdgdi91"
   },
   "outputs": [],
   "source": [
    "def sample_training_points(X, y, sample_size):\n",
    "    \"\"\"\n",
    "    Takes input features, targets and sample size, and returns random sample of size = sample_size\n",
    "\n",
    "    Args:\n",
    "    X : numpy array of shape (n_samples, n_features)\n",
    "    y : numpy array of shape (n_samples, 1)\n",
    "\n",
    "    Returns:\n",
    "    sampled_X : numpy array of shape (sample_size, n_features)\n",
    "    sampled_y : numpy array of shape (sample_size, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    sampled_X, sampled_y = None, None\n",
    "\n",
    "    ## TODO\n",
    "    indices = np.arange(X.shape[0])\n",
    "    sample_indices = np.random.choice(indices, sample_size, replace=False)\n",
    "    sampled_X, sampled_y = X[sample_indices], y[sample_indices]\n",
    "    ## END TODO\n",
    "\n",
    "    assert sampled_X.shape == (sample_size, X.shape[1]) and sampled_y.shape == (\n",
    "        sample_size,\n",
    "        1,\n",
    "    )\n",
    "\n",
    "    return sampled_X, sampled_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE4aGEKSf_mo"
   },
   "source": [
    "LOGISTIC REGRESSION CLASSIFIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmQHu4nYgksD"
   },
   "outputs": [],
   "source": [
    "def sigmoid(weights, bias, X):\n",
    "    \"\"\"\n",
    "    Implement logistic/sigmoid function\n",
    "\n",
    "    Args:\n",
    "    weights : numpy array of shape (n_dimension, 1)\n",
    "    bias : scalar\n",
    "    X : numpy array of shape (n_samples, n_dimension)\n",
    "\n",
    "    Returns:\n",
    "    Y_sigmoid : numpy array of shape (n_samples, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    Y_sigmoid = None\n",
    "\n",
    "    ## TODO\n",
    "    Y_sigmoid = 1 / (1 + np.exp(-np.dot(X, weights) - bias))\n",
    "    ## END TODO\n",
    "    assert Y_sigmoid.shape == (X.shape[0], 1)\n",
    "\n",
    "    return Y_sigmoid\n",
    "\n",
    "\n",
    "def cross_entropy_loss(weights, bias, X, y):\n",
    "    \"\"\"\n",
    "    Takes input features, weights, bias and target\n",
    "    and calculates binary cross entropy loss between y and predicted values of y.\n",
    "\n",
    "    Args:\n",
    "    weights : numpy array of shape (n_features, 1)\n",
    "    bias : scalar\n",
    "    X : numpy array of shape (n_samples, 1)\n",
    "    y : numpy array of shape (n_samples, 1)\n",
    "\n",
    "    Returns :\n",
    "    loss : single float value\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    ## TODO\n",
    "    sig = sigmoid(weights, bias, X)\n",
    "    loss = -(y * np.log(sig) + (1 - y) * np.log(1 - sig)).mean()\n",
    "    ## END TODO\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad(X, y, weights, bias):\n",
    "\n",
    "    \"\"\"\n",
    "    Return gradient for weights and biases\n",
    "\n",
    "    Args:\n",
    "    X : numpy array of shape (n_samples , n_dimension)\n",
    "    y : numpy array of shape (n_samples , 1)\n",
    "    weights : numpy array of shape (n_dimension , 1)\n",
    "    bias : scalar\n",
    "\n",
    "    Returns:\n",
    "    gradient : [dw , db]\n",
    "        dw - numpy array of shape (n_dimension , 1)\n",
    "        db - numpy array of shape (1, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    dw, db = None, None\n",
    "\n",
    "    ## TODO\n",
    "    diff = y - sigmoid(weights, bias, X)\n",
    "    dw = -np.array([(diff * X).mean(axis=0)]).T\n",
    "    db = -np.array([[diff.mean()]])\n",
    "    ## END TODO\n",
    "\n",
    "    assert db.shape == (1, 1)\n",
    "    assert dw.shape == weights.shape\n",
    "\n",
    "    gradient = [dw, db]\n",
    "\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def logistic_regression(X, y, epoch, lr, sample_size):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "\n",
    "    X : numpy array of input features of shape (n_samples, n_features)\n",
    "    y : numpy array of targets of shape (n_samples, 1)\n",
    "    epochs : number of iterations of training\n",
    "    lr : learning_rate\n",
    "    sample_size : batch_size for each iteration\n",
    "\n",
    "    Returns :\n",
    "    loss : list containing loss for each epoch\n",
    "    weights : numpy array of shape (n_features, 1)\n",
    "    bias : scalar\n",
    "    \"\"\"\n",
    "\n",
    "    loss, weights, bias = [], None, 0\n",
    "    # define weights and bias (w, b) initialize the weight\n",
    "\n",
    "    ## TODO\n",
    "    nf = X.shape[1]\n",
    "    weights = np.zeros((nf, 1))\n",
    "    for i in range(epoch):\n",
    "        sampled_X, sampled_y = sample_training_points(X, y, sample_size)\n",
    "        loss.append(cross_entropy_loss(weights, bias, sampled_X, sampled_y))\n",
    "        gradient = grad(sampled_X, sampled_y, weights, bias)\n",
    "        weights -= lr * gradient[0]\n",
    "        bias -= lr * gradient[1]\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    assert len(loss) == epoch and weights.shape == (X.shape[1], 1)\n",
    "\n",
    "    return [loss, weights, bias]\n",
    "\n",
    "\n",
    "def train_multi_class(X, Y):\n",
    "    \"\"\"\n",
    "    make multi class classifier using binary classification (1 vs all)\n",
    "\n",
    "    Args:\n",
    "    X : numpy array of input features of shape (n_samples, n_features)\n",
    "    Y : numpy array of input features of shape (n_samples, 1)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    param_list : list of param for all classifiers, where param = [weights, bias]\n",
    "    loss_list : list of lists of losses of all classifiers for all epochs\n",
    "    \"\"\"\n",
    "\n",
    "    digits = [1, 4, 7, 9]\n",
    "    param_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    # train i vs all weights and bias, where i in {1,4,7,9}\n",
    "    epochs, lr, sample_size = 500, 0.01, 120\n",
    "\n",
    "    for i in digits:\n",
    "        X_train, Y_train = get_data_for_class(X, Y, id=i)\n",
    "        output = logistic_regression(X_train, Y_train, epochs, lr, sample_size)\n",
    "        param_list.append(output[1:])\n",
    "        loss_list.append(output[0])\n",
    "\n",
    "    return param_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w96QeoELvelA"
   },
   "outputs": [],
   "source": [
    "def to_class(predicts):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    predicts : numpy array of shape (n_samples, 4)\n",
    "\n",
    "    Return:\n",
    "    predicted classes : numpy array of shape(num_samples, class_id), where class_id in {1,4,7,9}\n",
    "\n",
    "    \"\"\"\n",
    "    labels = {0:1, 1:4, 2:7, 3:9}\n",
    "    predicted_class = []\n",
    "    for i in range(predicts.shape[0]):\n",
    "        label = np.argmax(predicts[i])\n",
    "        predicted_class.append(labels[label])\n",
    "\n",
    "    return np.array(predicted_class).reshape(len(predicted_class),1)\n",
    "\n",
    "def prediction(param, X):\n",
    "    \"\"\"\n",
    "    It return the array of predicted class for all samples in X\n",
    "\n",
    "    Args:\n",
    "    param[0] = [w,b] of class 1 vs all\n",
    "    param[1] = [w,b] of class 4 vs all\n",
    "    param[2] = [w,b] of class 7 vs all\n",
    "    param[3] = [w,b] of class 9 vs all\n",
    "\n",
    "    X = numpy array of input features, shape (n_samples , 784)\n",
    "\n",
    "    Returns:\n",
    "    predicts = numpy array for shape (n_samples , 4),\n",
    "        where predicts[i][j] is probability that sample i belongs to class j when using classifier j vs all\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    predicts = None\n",
    "\n",
    "    ## TODO\n",
    "    predicts = np.zeros((X.shape[0], 4))\n",
    "    for i in range(4):\n",
    "        predicts[:, i] = np.squeeze(np.dot(X, param[i][0]) + param[i][1])\n",
    "    ## END TODO\n",
    "\n",
    "    assert predicts.shape == (X.shape[0], 4)\n",
    "\n",
    "    return predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2om8E02u579X"
   },
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y_true):\n",
    "\n",
    "    \"\"\"\n",
    "    This functions calculates accuracy for -\n",
    "      1) each of the 4 class\n",
    "      2) whole dataset\n",
    "      Note : Do not report in percentage\n",
    "\n",
    "    Args:\n",
    "    Y_pred : Predicted class labels, numpy array of shape (n_samples , 1)\n",
    "    Y_true : True/Actual class labels, numpy array of shape (n_samples , 1)\n",
    "\n",
    "    Returns:\n",
    "    list of length 5, 1 st value is overall accuracy followed by accuracy of all 4 individual classifies\n",
    "    \"\"\"\n",
    "\n",
    "    total_accuracy = np.sum(Y_pred == Y_true) / Y_true.shape[0]\n",
    "    accuracy_1, accuracy_4, accuracy_7, accuracy_9 = 0, 0, 0, 0\n",
    "    # TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    print(\"Total Accuray : \", total_accuracy)\n",
    "    print(\"Accuray class 1 : \", accuracy_1)\n",
    "    print(\"Accuray class 4 : \", accuracy_4)\n",
    "    print(\"Accuray class 7 : \", accuracy_7)\n",
    "    print(\"Accuray class 9 : \", accuracy_9)\n",
    "\n",
    "    return [total_accuracy, accuracy_1, accuracy_4, accuracy_7, accuracy_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl9_jRs0bsU5"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(Y_pred, Y_true):\n",
    "\n",
    "    \"\"\"\n",
    "    This functions calculates precision, recall and f1-score for -\n",
    "      1) each of the 4 class\n",
    "      2) whole dataset\n",
    "      Note : Do not report in percentage\n",
    "\n",
    "    Args:\n",
    "    Y_pred : Predicted class labels, numpy array of shape (n_samples , 1)\n",
    "    Y_true : True/Actual class labels, numpy array of shape (n_samples , 1)\n",
    "\n",
    "    Returns:\n",
    "    (precision , recall , f1_score) : a tuple of 3 lists i.e. precision, recall and f1_score where\n",
    "            precision : list of length 5, 1 st value is overall precision followed by precision of all 4 individual classes\n",
    "            recall : list of length 5, 1 st value is overall recall followed by recall of all 4 individual classes\n",
    "            f1_score : list of length 5, 1 st value is overall f1-score followed by f1-score of all 4 individual classes\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall, f1_score = [], [], []\n",
    "\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "\n",
    "    assert len(precision) == 5 and len(recall) == 5 and len(f1_score) == 5\n",
    "\n",
    "    return (precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q82oEeRF-guz"
   },
   "outputs": [],
   "source": [
    "def plot_training_loss(loss_list):\n",
    "    labels = [\"1\", \"4\", \"7\", \"9\"]\n",
    "    for ids, loss in enumerate(loss_list):\n",
    "        plt.plot(loss, label=labels[ids])\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Training Loss of each class (1 vs all)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyVYAPhtm_At"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = split_data(X, Y)\n",
    "param_lists, loss_list = train_multi_class(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9kL6ByDgZyI"
   },
   "source": [
    "## Plot curve for loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC3lGdUy-jek"
   },
   "outputs": [],
   "source": [
    "plot_training_loss(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkva3PIgcsu"
   },
   "source": [
    "## Calculate accuracy, precision , recall and F1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNTZuQF8nDTD"
   },
   "outputs": [],
   "source": [
    "Y_pred = to_class(prediction(param_lists, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slH97B8L0iHR"
   },
   "outputs": [],
   "source": [
    "accuracy(Y_pred, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLSImjxiweiV"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score = calculate_metrics(Y_pred, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_3.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aea86b65f3ce5091a535cd6a4329dd553b3d175d52cb47abc7b8a422461c3cbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
